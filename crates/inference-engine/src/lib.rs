use std::sync::Arc;

use anyhow::Context;
use sauropod_inference_engine_api::LlmModel;

mod model_file_selector;

/// A pointer to a model.
pub type ModelPointer = Arc<dyn LlmModel + Send + 'static>;

pub struct Model {
    /// The pointer to the loaded model.
    pub underlying_model: ModelPointer,
    /// Prompt template for the model.
    pub chat_template: sauropod_prompt_templates::PromptTemplate,
    /// The config for the model.
    pub model_config: sauropod_config::ModelConfig,
    /// Whether the model supports audio input.
    pub supports_audio_input: bool,
    /// Whether the model supports image input.
    pub supports_image_input: bool,
}

impl Model {
    pub fn new(
        underlying_model: ModelPointer,
        model_config: sauropod_config::ModelConfig,
    ) -> anyhow::Result<Self> {
        let chat_template = sauropod_prompt_templates::PromptTemplate::new(
            model_config
                .chat_template
                .clone()
                .unwrap_or_else(|| underlying_model.get_model_chat_template().to_string()),
        )?;

        Ok(Self {
            underlying_model,
            chat_template,
            model_config,
            supports_audio_input: false,
            supports_image_input: false,
        })
    }

    /// Get the system prompt for the model.
    pub fn get_system_prompt(&self) -> Option<&str> {
        self.model_config.system_prompt.as_deref()
    }

    /// Generate responses using the underlying model.
    pub async fn generate_stream(
        self: Arc<Self>,
        input: sauropod_openai_api::CreateResponse,
        render_context: sauropod_prompt_templates::RenderContext,
    ) -> anyhow::Result<sauropod_inference_engine_api::ResponseStream> {
        tracing::debug!("Generating response for input: {input:#?}");
        let model = self.underlying_model.clone();
        let response = sauropod_inference_engine_api::make_response(&input);

        let rendered_prompt = self
            .chat_template
            .render(&render_context)
            .with_context(|| {
                format!("Failed to render chat template for context: {render_context:#?}")
            })?;
        let sampler_properties =
            sauropod_inference_engine_api::SamplerProperties::new(&response, &self.model_config);
        let mut response_stream_creator = sauropod_inference_engine_api::ResponseStreamCreator::new(
            sauropod_output_parser::get_model_parser(model.get_model_type()),
            response,
        );
        let sauropod_inference_engine_api::GenerateFromTextResponse {
            stream,
            input_token_count,
        } = model
            .generate_from_text(
                sampler_properties,
                rendered_prompt,
                render_context.multimodal_data,
            )
            .await
            .context("Generating token stream")?;

        response_stream_creator
            .response
            .usage
            .as_mut()
            .unwrap()
            .input_tokens = input_token_count;
        let stream = async_stream::stream! {
            for await part in stream {
                match part {
                    Ok(part) => {
                        for event in response_stream_creator.push_part(part) {
                            yield Ok(event);
                        }
                    }
                    Err(e) => {
                        yield Err(e);
                        break;
                    }
                };
            }

            // None means the sender has closed the channel
            if response_stream_creator.is_empty() {
                for event in response_stream_creator.push_text("".to_string()) {
                    yield Ok(event);
                }
            }

            for event in response_stream_creator.finish() {
                yield Ok(event);
            }
        };

        Ok(Box::pin(stream) as sauropod_inference_engine_api::ResponseStream)
    }

    /// Generate responses using the underlying model.
    pub async fn generate(
        self: Arc<Self>,
        input: sauropod_openai_api::CreateResponse,
        render_context: sauropod_prompt_templates::RenderContext,
    ) -> anyhow::Result<sauropod_openai_api::Response> {
        use tokio_stream::StreamExt as _;

        let stream = self.generate_stream(input, render_context).await?;

        let completed = stream
            .filter_map(|x| match x {
                Ok(sauropod_openai_api::ResponseStreamEvent::ResponseCompletedEvent {
                    response,
                    ..
                }) => Some(Ok(response)),
                Err(e) => Some(Err(e)),
                _ => None,
            })
            .next()
            .await;
        match completed {
            Some(response) => Ok(response?),
            None => anyhow::bail!("No ResponseCompletedEvent was generated by the model"),
        }
    }
}

/// Get the model path from a model source.
///
/// This may download files from Hugging Face.
pub async fn get_model_path(
    model_source: &sauropod_config::ConfigModelSource,
) -> anyhow::Result<sauropod_inference_engine_api::ModelPath> {
    let selected_file = match model_source {
        sauropod_config::ConfigModelSource::LocalPath(path) => {
            let path = std::path::PathBuf::from(path);
            if path.is_dir() {
                // Collect all files in the directory
                let files: Result<Vec<String>, _> = path
                    .read_dir()?
                    .map(|entry| entry.map(|e| e.file_name().to_string_lossy().into_owned()))
                    .collect();

                let files = files?;

                if let Some(selected_file) = model_file_selector::select_file(&files, None) {
                    std::path::PathBuf::from(selected_file)
                } else {
                    path.clone()
                }
            } else {
                path.clone()
            }
        }
        sauropod_config::ConfigModelSource::HuggingFace(hf_repo) => {
            let interface = sauropod_huggingface::RepositoryInterface::new()?;
            let metadata = interface.get_repository_metadata(hf_repo).await?;
            let selected_file = match &hf_repo.path_or_quantization {
                Some(sauropod_config::PathOrQuantization::FilePath { file }) => file.to_string(),
                Some(sauropod_config::PathOrQuantization::Quantization { quantization }) => {
                    let repo_name = if let Some(pair) = hf_repo.repo.rsplit_once('/') {
                        pair.1
                    } else {
                        hf_repo.repo.as_str()
                    };

                    let mut search_paths = Vec::with_capacity(3);
                    search_paths.push(format!("{repo_name}-{quantization}.gguf"));
                    if let Some(repo_name_without_suffix) = repo_name.strip_suffix("-GGUF") {
                        search_paths
                            .push(format!("{repo_name_without_suffix}-{quantization}.gguf"));
                    }
                    search_paths.push(format!("{quantization}.gguf"));

                    let mut maybe_local_file = None;
                    // Check for an already cached file
                    for file_name in search_paths {
                        tracing::debug!("Looking for {file_name} in the cache");
                        if metadata.get_path(&file_name).is_some() {
                            maybe_local_file = Some(file_name);
                            break;
                        }
                    }

                    if let Some(local_file) = maybe_local_file {
                        local_file
                    } else {
                        // If we can't find a pre-cached file then fetch the list of files in the repository and try to find a match
                        let all_files = metadata.get_all_files().await?;
                        model_file_selector::select_file(&all_files, Some(quantization.as_str()))
                            .ok_or_else(|| {
                                anyhow::anyhow!("Failed to select a model file for {hf_repo}")
                            })?
                    }
                }
                None => {
                    let all_files = metadata.get_all_files().await?;
                    model_file_selector::select_file(&all_files, None).ok_or_else(|| {
                        anyhow::anyhow!("Failed to select a model file for {hf_repo}")
                    })?
                }
            };
            let _downloaded_files = metadata.download(&[&selected_file]).await?;

            let Some(file_path) = metadata.get_path(&selected_file) else {
                anyhow::bail!(
                    "Failed to get path for selected file {selected_file} in the repo {hf_repo}"
                );
            };
            file_path
        }
    };

    if selected_file.extension() == Some(std::ffi::OsStr::new("engine")) {
        Ok(sauropod_inference_engine_api::ModelPath::TensorRT(
            selected_file,
        ))
    } else if selected_file.extension() == Some(std::ffi::OsStr::new("gguf")) {
        Ok(sauropod_inference_engine_api::ModelPath::GGUF(
            selected_file,
        ))
    } else {
        anyhow::bail!(
            "Unsupported model file extension {:?} in path {}",
            selected_file.extension(),
            selected_file.display()
        );
    }
}

/// Load a model.
pub async fn load_model(
    name: String,
    model_path: &sauropod_inference_engine_api::ModelPath,
    projector_model_path: Option<&sauropod_config::ConfigModelSource>,
) -> anyhow::Result<ModelPointer> {
    let projector_model_path = match projector_model_path {
        Some(source) => Some(sauropod_huggingface::download_file(source).await?),
        None => None,
    };
    match model_path {
        sauropod_inference_engine_api::ModelPath::TensorRT(_) => {
            anyhow::bail!("TensorRT-LLM support is not enabled in this build");
        }
        sauropod_inference_engine_api::ModelPath::GGUF(path) => Ok(Arc::new(
            sauropod_llama_cpp::ModelInferenceThread::from_file(
                name,
                path,
                projector_model_path.as_deref(),
            )
            .await?,
        ) as ModelPointer),
    }
}
