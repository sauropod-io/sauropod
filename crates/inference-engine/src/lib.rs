use std::sync::Arc;

use anyhow::Context;
use sauropod_inference_engine_api::LlmModel;

mod model_file_selector;

/// A pointer to a model.
pub type ModelPointer = Arc<dyn LlmModel + Send + 'static>;

pub struct Model {
    /// The pointer to the loaded model.
    pub underlying_model: ModelPointer,
    /// Prompt template for the model.
    pub chat_template: sauropod_prompt_templates::PromptTemplate,
    /// The config for the model.
    pub model_config: sauropod_config::ModelConfig,
    /// Whether the model supports audio input.
    pub supports_audio_input: bool,
    /// Whether the model supports image input.
    pub supports_image_input: bool,
}

impl Model {
    pub fn new(
        underlying_model: ModelPointer,
        model_config: sauropod_config::ModelConfig,
    ) -> anyhow::Result<Self> {
        let chat_template = sauropod_prompt_templates::PromptTemplate::new(
            underlying_model.get_model_chat_template().to_string(),
        )?;

        Ok(Self {
            underlying_model,
            chat_template,
            model_config,
            supports_audio_input: false,
            supports_image_input: false,
        })
    }

    /// Get the system prompt for the model.
    pub fn get_system_prompt(&self) -> Option<&str> {
        self.model_config.system_prompt.as_deref()
    }

    /// Generate responses using the underlying model.
    pub async fn generate_stream(
        self: Arc<Self>,
        input: sauropod_openai_api::CreateResponse,
        render_context: sauropod_prompt_templates::RenderContext,
    ) -> anyhow::Result<sauropod_inference_engine_api::ResponseStream> {
        tracing::debug!("Generating response for input: {input:#?}");
        let model = self.underlying_model.clone();
        let response = sauropod_inference_engine_api::make_response(&input);

        let rendered_prompt = self
            .chat_template
            .render(&render_context)
            .with_context(|| {
                format!("Failed to render chat template for context: {render_context:#?}")
            })?;
        let sampler_properties =
            sauropod_inference_engine_api::SamplerProperties::new(&response, &self.model_config);
        let mut response_stream_creator = sauropod_inference_engine_api::ResponseStreamCreator::new(
            sauropod_output_parser::get_model_parser(model.get_model_type()),
            response,
        );
        let stream = model
            .generate_from_text(sampler_properties, rendered_prompt)
            .await
            .context("Generating token stream")?;

        let stream = async_stream::stream! {
            for await part in stream {
                match part {
                    Ok(part) => {
                        for event in response_stream_creator.push_text(part) {
                            yield Ok(event);
                        }
                    }
                    Err(e) => {
                        yield Err(e);
                        break;
                    }
                };
            }

            // None means the sender has closed the channel
            if response_stream_creator.is_empty() {
                for event in response_stream_creator.push_text("".to_string()) {
                    yield Ok(event);
                }
            }

            for event in response_stream_creator.finish() {
                yield Ok(event);
            }
        };

        Ok(Box::pin(stream) as sauropod_inference_engine_api::ResponseStream)
    }

    /// Generate responses using the underlying model.
    pub async fn generate(
        self: Arc<Self>,
        input: sauropod_openai_api::CreateResponse,
        render_context: sauropod_prompt_templates::RenderContext,
    ) -> anyhow::Result<sauropod_openai_api::Response> {
        use tokio_stream::StreamExt as _;

        let stream = self.generate_stream(input, render_context).await?;

        let completed = stream
            .filter_map(|x| match x {
                Ok(sauropod_openai_api::ResponseStreamEvent::ResponseCompletedEvent {
                    response,
                    ..
                }) => Some(response),
                _ => None,
            })
            .next()
            .await;
        match completed {
            Some(response) => Ok(response),
            None => anyhow::bail!("No ResponseCompletedEvent was generated by the model"),
        }
    }
}

/// Get the model path from a model source.
///
/// This may download files from Hugging Face.
pub async fn get_model_path(
    model_source: &sauropod_config::ConfigModelSource,
) -> anyhow::Result<sauropod_inference_engine_api::ModelPath> {
    let selected_file = match model_source {
        sauropod_config::ConfigModelSource::LocalPath(path) => {
            let path = std::path::PathBuf::from(path);
            if path.is_dir() {
                // Collect all files in the directory
                let files: Result<Vec<String>, _> = path
                    .read_dir()?
                    .map(|entry| entry.map(|e| e.file_name().to_string_lossy().into_owned()))
                    .collect();

                let files = files?;

                if let Some(selected_file) = model_file_selector::select_file(&files, None) {
                    std::path::PathBuf::from(selected_file)
                } else {
                    path.clone()
                }
            } else {
                path.clone()
            }
        }
        sauropod_config::ConfigModelSource::HuggingFace(hf_repo) => {
            let interface = sauropod_huggingface::RepositoryInterface::new()?;
            let metadata = interface.get_repository_metadata(hf_repo).await?;
            let selected_file = match &hf_repo.path_or_quantization {
                Some(sauropod_config::PathOrQuantization::Quantization { quantization }) => {
                    model_file_selector::select_file(&metadata.files, Some(quantization.as_str()))
                        .unwrap_or(".")
                }
                Some(sauropod_config::PathOrQuantization::FilePath { file }) => file.as_str(),
                None => model_file_selector::select_file(&metadata.files, None).unwrap_or("."),
            };
            let mut files_to_download = Vec::with_capacity(1);

            for file in &metadata.files {
                if file.starts_with(selected_file)
                    || file.ends_with(".json")
                    || selected_file == "."
                {
                    files_to_download.push(file.as_str());
                }
            }
            let _downloaded_files = metadata.download(&files_to_download).await?;

            let Some(file_path) = metadata.get_path(selected_file) else {
                anyhow::bail!("Failed to get path for selected file: {}", selected_file);
            };
            file_path
        }
    };

    if selected_file.extension() == Some(std::ffi::OsStr::new("engine")) {
        Ok(sauropod_inference_engine_api::ModelPath::TensorRT(
            selected_file,
        ))
    } else if selected_file.extension() == Some(std::ffi::OsStr::new("gguf")) {
        Ok(sauropod_inference_engine_api::ModelPath::GGUF(
            selected_file,
        ))
    } else {
        anyhow::bail!(
            "Unsupported model file extension {:?} in path {}",
            selected_file.extension(),
            selected_file.display()
        );
    }
}

/// Load a model.
pub async fn load_model(
    name: String,
    model_path: &sauropod_inference_engine_api::ModelPath,
) -> anyhow::Result<ModelPointer> {
    match model_path {
        sauropod_inference_engine_api::ModelPath::TensorRT(_) => {
            anyhow::bail!("TensorRT-LLM support is not enabled in this build");
        }
        sauropod_inference_engine_api::ModelPath::GGUF(path) => Ok(Arc::new(
            sauropod_llama_cpp::ModelInferenceThread::from_file(name, path).await?,
        ) as ModelPointer),
    }
}
