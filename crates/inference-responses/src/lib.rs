use axum::response::sse::Event as SseEvent;
use axum::{response::IntoResponse, response::Sse};

use sauropod_openai_api::{
    CreateResponse, Response, ResponseError, ResponseErrorCode, ResponseStatus,
};

mod routes;
pub use routes::*;

/// A generated response and the data used to create it.
#[derive(Clone)]
struct ResponseData {
    /// The inputs used to create the response.
    input: Option<sauropod_openai_api::CreateResponseInput>,
    /// The response generated by the model.
    response: Response,
}

fn in_memory_responses()
-> &'static tokio::sync::RwLock<std::collections::HashMap<String, ResponseData>> {
    // This is a placeholder for the in-memory responses.
    // In a real application, this would query a database or other storage.
    static RESPONSES: std::sync::OnceLock<
        tokio::sync::RwLock<std::collections::HashMap<String, ResponseData>>,
    > = std::sync::OnceLock::new();
    RESPONSES.get_or_init(|| tokio::sync::RwLock::new(std::collections::HashMap::new()))
}

/// Create a `Response` with an error message.
fn response_with_error(
    request: &CreateResponse,
    message: String,
) -> axum::Json<sauropod_openai_api::Response> {
    let mut response = sauropod_inference_engine_api::make_response(request);
    response.status = Some(ResponseStatus::Failed);
    response.error = Some(ResponseError {
        message,
        code: ResponseErrorCode::ServerError,
    });
    axum::Json(response)
}

/// Merge the current request with the previous responses.
///
/// The responses in `previous` are ordered from the most recent to the oldest.
fn merge_responses(request: &mut CreateResponse, previous: Vec<ResponseData>) {
    // If no instructions are provided in the request, try to use the instructions from the previous responses.
    if request.instructions.is_none() {
        request.instructions = previous
            .iter()
            .find_map(|x| match &x.response.instructions {
                Some(sauropod_openai_api::ResponseInstructions::Variant0(instructions)) => {
                    Some(instructions.clone())
                }
                Some(sauropod_openai_api::ResponseInstructions::Variant1(_)) => {
                    panic!("Item-based instructions are not supported yet")
                }
                None => None,
            });
    }

    let mut messages = Vec::new();
    for response in previous.into_iter().rev() {
        match response.input {
            Some(sauropod_openai_api::CreateResponseInput::Variant0(content)) => {
                messages.push(sauropod_openai_api::InputItem::EasyInputMessage(
                    sauropod_openai_api::EasyInputMessage {
                        content: sauropod_openai_api::EasyInputMessageContent::Variant0(
                            content.clone(),
                        ),
                        role: sauropod_openai_api::EasyInputMessageRole::User,
                        r#type: Some(sauropod_openai_api::EasyInputMessageType::Message),
                    },
                ));
            }
            Some(sauropod_openai_api::CreateResponseInput::Variant1(items)) => {
                messages.extend(items.iter().cloned());
            }
            None => {}
        }
        for output in response.response.output {
            messages.push(sauropod_openai_api::InputItem::Item(output.into()));
        }
    }

    match &request.input {
        Some(sauropod_openai_api::CreateResponseInput::Variant0(content)) => {
            messages.push(sauropod_openai_api::InputItem::EasyInputMessage(
                sauropod_openai_api::EasyInputMessage {
                    content: sauropod_openai_api::EasyInputMessageContent::Variant0(
                        content.clone(),
                    ),
                    role: sauropod_openai_api::EasyInputMessageRole::User,
                    r#type: Some(sauropod_openai_api::EasyInputMessageType::Message),
                },
            ));
        }
        Some(sauropod_openai_api::CreateResponseInput::Variant1(items)) => {
            messages.extend(items.iter().cloned());
        }
        None => {}
    }

    request.input = Some(sauropod_openai_api::CreateResponseInput::Variant1(messages));
}

pub async fn create_response_impl(
    loaded_models: std::sync::Arc<sauropod_global_state::GlobalState>,
    request: CreateResponse,
) -> anyhow::Result<axum::response::Response> {
    tracing::debug!("create response request: {:#?}", request);
    let model_name = match serde_json::to_value(&request.response_properties.model) {
        Ok(serde_json::Value::String(model_name)) => model_name,
        Ok(x) => {
            return Ok(
                response_with_error(&request, format!("Invalid model name: {x}")).into_response(),
            );
        }
        Err(e) => {
            return Ok(
                response_with_error(&request, format!("Invalid model name: {e}")).into_response(),
            );
        }
    };

    let Some(model) = loaded_models.get_model(&model_name).await else {
        return Ok(
            response_with_error(&request, format!("Model '{model_name}' not found"))
                .into_response(),
        );
    };

    let mut maybe_previous_response_id = &request.response_properties.previous_response_id;
    let mut previous_responses = Vec::<ResponseData>::with_capacity(2);
    if maybe_previous_response_id.is_some() {
        // If a previous response ID is provided, check if it exists in the in-memory responses.
        let responses = in_memory_responses();
        let responses_guard = responses.read().await;

        while let Some(previous_response_id) = &maybe_previous_response_id {
            if let Some(previous_response) = responses_guard.get(previous_response_id) {
                previous_responses.push(previous_response.clone());
                maybe_previous_response_id = &previous_response
                    .response
                    .response_properties
                    .previous_response_id;
            } else {
                return Ok(response_with_error(
                    &request,
                    format!("Previous response ID '{previous_response_id}' not found"),
                )
                .into_response());
            }
        }
    }

    let mut merged_request = request.clone();
    merge_responses(&mut merged_request, previous_responses);
    let render_context = sauropod_prompt_templates::RenderContext::from_create_response(
        &merged_request,
        model.get_system_prompt(),
    )?;

    tracing::debug!("Merged request: {:#?}", merged_request);

    let store = request.store.unwrap_or(false);
    if request.stream.unwrap_or(false) {
        let event_stream = model
            .generate_stream(request.clone(), render_context)
            .await?;
        let mapped_stream = async_stream::stream! {
            for await event in event_stream {
                match event {
                    Ok(sauropod_openai_api::ResponseStreamEvent::ResponseCompletedEvent {
                        response,
                        sequence_number,
                    }) => {
                        // Store the response if requested
                        if store {
                            let responses = in_memory_responses();
                            let mut responses_guard = responses.write().await;
                            responses_guard.insert(response.id.clone(), ResponseData {
                                    input: request.input.clone(),
                                    response: response.clone(),
                                });
                            }

                        yield Ok(SseEvent::default()
                            .json_data(
                                sauropod_openai_api::ResponseStreamEvent::ResponseCompletedEvent {
                                    response,
                                    sequence_number,
                                },
                            ).unwrap())
                    }

                    Ok(data) => yield Ok(SseEvent::default().json_data(data).unwrap()),
                    Err(e) => yield Err(e),
                }
            }
        };
        Ok(Sse::new(mapped_stream).into_response())
    } else {
        let response = model.generate(request.clone(), render_context).await?;

        // Store the response if requested
        if store {
            let responses = in_memory_responses();
            let mut responses_guard = responses.write().await;
            responses_guard.insert(
                response.id.clone(),
                ResponseData {
                    input: request.input.clone(),
                    response: response.clone(),
                },
            );
        }

        Ok(axum::Json(response).into_response())
    }
}
